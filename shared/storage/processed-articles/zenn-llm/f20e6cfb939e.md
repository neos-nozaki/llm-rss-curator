---
title: LLM で使われる位置情報のベクトル化について調べてみる
url: https://zenn.dev/kawara_y/articles/27f69346c851f7
author: かわら
published: Mon, 10 Nov 2025 18:13:52 GMT
feed: zenn-llm
---

## 🎯 この記事の要点

位置情報のベクトル化は、自己注意機構を持つモデルで文中の単語の位置を考慮するために重要です。この記事では、絶対位置と相対位置のエンコーディング、RoPE、ALiBiといった位置情報の表現手法を比較し、それぞれの利点と欠点を解説しています。

## 📚 背景・コンテキスト

自己注意機構を持つモデル（例：Transformer）は、入力を順序に依存しない集合として扱うため、単語の位置情報が失われがちです。位置情報のベクトル化は、この問題を解決し、文の意味を正しく捉えるために不可欠です。

## 🔍 技術的な詳細

- **絶対位置エンコーディング**: 各単語の位置を固定のベクトルで表現し、sinとcos関数を用いて周期的に位置をエンコードします。
- **相対位置エンコーディング**: 単語間の相対位置をベクトル化し、自己注意機構の計算時に使用することで、長文でも性能を維持します。
- **Rotary Position Embedding (RoPE)**: 回転行列を用いて位置を表現し、絶対位置と相対位置の両方を自然に扱えるようにします。
- **Attention with Linear Biases (ALiBi)**: 注意スコアに線形のバイアスを追加するシンプルな手法で、追加のパラメータなしに位置情報を表現します。

## 💡 実践的な示唆

- 長文を扱う場合は、相対位置エンコーディングやRoPEを検討すると良いでしょう。これらは長文でも性能を維持することが示されています。
- モデルのパラメータを増やさずに位置情報を考慮したい場合は、ALiBiが有効です。
- 絶対位置と相対位置のエンコーディングは、用途によって使い分けることが重要です。特に、モデルの設計やタスクの特性に応じて適切な手法を選択することが求められます。

## 👥 対象読者

- 自己注意機構を持つモデル（Transformerなど）を扱う機械学習エンジニア
- 自然言語処理における位置情報の重要性を理解したい研究者や学生
- モデルの性能向上を目指すデータサイエンティスト

## 🏷️ キーワード

位置情報エンコーディング, 絶対位置, 相対位置, Transformer, 自己注意機構, RoPE, ALiBi, 自然言語処理, 機械学習